{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "for module in [pd, np, torch]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "ch = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '北', '京', '天', '安', '门']\n",
      "['我', '爱', '北京', '天安门']\n"
     ]
    }
   ],
   "source": [
    "def charTokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "print(charTokenize('我爱北京天安门'))\n",
    "\n",
    "def cnTokenize(text):\n",
    "    return [tok.text for tok in ch(text)]\n",
    "\n",
    "print(cnTokenize('我爱北京天安门'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocab...\n",
      "<class 'torchtext.vocab.vocab.Vocab'> 7929\n",
      "([1, 25, 278, 349, 598, 97, 225, 335, 2], [2.0])\n",
      "Applied transform...\n",
      "Afte batch ...\n",
      "After seperate batch ...\n",
      "After apply padding ...\n",
      "<class 'torch.Tensor'> torch.Size([8, 579])\n",
      "<class 'torch.Tensor'> torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './data/train.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([train_data_path])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)\n",
    "\n",
    "def extract_attributes(row):\n",
    "    return row[1], row[2]\n",
    "data_pipe = data_pipe.map(extract_attributes)\n",
    "\n",
    "def get_tokens(data_iter, tokenizer):\n",
    "    for _, text in data_iter:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        get_tokens(data_iter, tokenizer),\n",
    "        min_freq=2,\n",
    "        specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
    "        special_first=True\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(data_pipe, charTokenize)\n",
    "print('Built vocab...')\n",
    "print(type(vocab), len(vocab))\n",
    "\n",
    "def getTransform(vocab):\n",
    "    text_transform = T.Sequential(\n",
    "        T.VocabTransform(vocab),\n",
    "        T.AddToken(1, begin=True),\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform\n",
    "\n",
    "def apply_transform(sample):\n",
    "    text_transformer = getTransform(vocab)\n",
    "    tokenized_text = charTokenize(sample[1])\n",
    "    return text_transformer(tokenized_text), [float(sample[0])]\n",
    "\n",
    "print(apply_transform(('2',\"我爱北京天安门\")))\n",
    "\n",
    "data_pipe = data_pipe.map(apply_transform)\n",
    "print('Applied transform...')\n",
    "#for sample in data_pipe:\n",
    "#    print(sample)\n",
    "#    break\n",
    "\n",
    "def sortBucket(bucket):\n",
    "    return sorted(bucket, key=lambda x: len(x[0]))\n",
    "\n",
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 8, \n",
    "    batch_num=5,  # batch_num is the number of batches to keep in a bucket\n",
    "    bucket_num=1, # bucket_num is the number of buckets\n",
    "    use_in_batch_shuffle=False, \n",
    "    sort_key=sortBucket\n",
    ")\n",
    "\n",
    "print('Afte batch ...')\n",
    "#for batch in data_pipe:\n",
    "#    print(batch)\n",
    "#    print(len(batch))\n",
    "#    break\n",
    "\n",
    "\n",
    "def separate_batch(batch):\n",
    "    '''\n",
    "    Inputs: [(text1, label1), (text2, label2), ...]\n",
    "    Outputs: ([text1, text2, ...], [label1, label2, ...])\n",
    "    '''\n",
    "    texts, labels = zip(*batch)\n",
    "    return texts, labels\n",
    "\n",
    "data_pipe = data_pipe.map(separate_batch)\n",
    "print('After seperate batch ...')\n",
    "#for texts, labels in data_pipe:\n",
    "#    print(len(texts), texts)\n",
    "#    print(len(labels), labels)\n",
    "#    break\n",
    "\n",
    "\n",
    "def apply_padding(sample):\n",
    "    return (T.ToTensor(0)(list(sample[0])), torch.tensor(list(sample[1])))\n",
    "\n",
    "data_pipe = data_pipe.map(apply_padding)\n",
    "print('After apply padding ...')\n",
    "for texts, labels in data_pipe:\n",
    "    print(type(texts), texts.shape)\n",
    "    print(type(labels), labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressionModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, embedding_pretrained=None) -> None:\n",
    "        super(LSTMRegressionModel, self).__init__()\n",
    "        if embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=vocab_size-2)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, embedding_size]\n",
    "        x = self.embedding(inputs)\n",
    "        # [batch_size, seq_len, embed_dim] -> [num_layers, batch_size, hidden_size]\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = x[-1, :, :]\n",
    "        # [batch_size, hidden_size] -> [batch_size, 1]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataset, 1):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if batch * len(X) % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch) * len(X)\n",
    "            print(f\"\\tbatch: [{current:>5d}], loss: {loss:>7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\tbatch: [  800], loss: 775.336182\n",
      "\tbatch: [ 1600], loss: 914.114441\n",
      "\tbatch: [ 2400], loss: 805.815674\n",
      "\tbatch: [ 3200], loss: 878.371582\n",
      "\tbatch: [ 4000], loss: 1815.596802\n",
      "\tbatch: [ 4800], loss: 1793.880005\n",
      "\tbatch: [ 5600], loss: 1074.542969\n",
      "\tbatch: [ 6400], loss: 596.021606\n",
      "\tbatch: [ 7200], loss: 1499.280884\n",
      "\tbatch: [ 8000], loss: 1283.532471\n",
      "\tbatch: [ 8800], loss: 1356.803101\n",
      "\tbatch: [ 9600], loss: 633.639221\n",
      "\tbatch: [10400], loss: 1184.123169\n",
      "\tbatch: [11200], loss: 1086.621826\n",
      "\tbatch: [12000], loss: 1517.476074\n",
      "\tbatch: [12800], loss: 877.793884\n",
      "\tbatch: [13600], loss: 1541.178223\n",
      "\tbatch: [14400], loss: 954.177002\n",
      "\tbatch: [15200], loss: 890.035583\n",
      "\tbatch: [16000], loss: 1330.868408\n",
      "\tbatch: [16800], loss: 960.231079\n",
      "\tbatch: [17600], loss: 1212.364868\n",
      "\tbatch: [18400], loss: 1075.360962\n",
      "\tbatch: [19200], loss: 1364.616943\n",
      "\tbatch: [20000], loss: 994.610840\n",
      "\tbatch: [20800], loss: 1004.119995\n",
      "\tbatch: [21600], loss: 1377.440918\n",
      "\tbatch: [22400], loss: 1106.871826\n",
      "Epoch: 0, escaplse     157.98 seconds\n",
      "Epoch: 1\n",
      "\tbatch: [  800], loss: 1261.809204\n",
      "\tbatch: [ 1600], loss: 2057.321777\n",
      "\tbatch: [ 2400], loss: 1487.393677\n",
      "\tbatch: [ 3200], loss: 1498.196045\n",
      "\tbatch: [ 4000], loss: 1112.856323\n",
      "\tbatch: [ 4800], loss: 1143.110107\n",
      "\tbatch: [ 5600], loss: 822.954651\n",
      "\tbatch: [ 6400], loss: 1481.151489\n",
      "\tbatch: [ 7200], loss: 1619.982666\n",
      "\tbatch: [ 8000], loss: 1213.394775\n",
      "\tbatch: [ 8800], loss: 895.966980\n",
      "\tbatch: [ 9600], loss: 1362.629761\n",
      "\tbatch: [10400], loss: 1380.470825\n",
      "\tbatch: [11200], loss: 1129.673828\n",
      "\tbatch: [12000], loss: 856.304382\n",
      "\tbatch: [12800], loss: 1573.241577\n",
      "\tbatch: [13600], loss: 2012.156006\n",
      "\tbatch: [14400], loss: 1172.663330\n",
      "\tbatch: [15200], loss: 1541.021729\n",
      "\tbatch: [16000], loss: 1135.256592\n",
      "\tbatch: [16800], loss: 1187.338013\n",
      "\tbatch: [17600], loss: 731.958435\n",
      "\tbatch: [18400], loss: 1240.704834\n",
      "\tbatch: [19200], loss: 1044.535522\n",
      "\tbatch: [20000], loss: 1645.946411\n",
      "\tbatch: [20800], loss: 1033.219360\n",
      "\tbatch: [21600], loss: 1501.905518\n",
      "\tbatch: [22400], loss: 1106.806641\n",
      "Epoch: 1, escaplse     153.82 seconds\n",
      "Epoch: 2\n",
      "\tbatch: [  800], loss: 1083.211182\n",
      "\tbatch: [ 1600], loss: 788.750366\n",
      "\tbatch: [ 2400], loss: 1147.243164\n",
      "\tbatch: [ 3200], loss: 1329.440430\n",
      "\tbatch: [ 4000], loss: 1444.139893\n",
      "\tbatch: [ 4800], loss: 1083.997070\n",
      "\tbatch: [ 5600], loss: 1081.011108\n",
      "\tbatch: [ 6400], loss: 1540.567993\n",
      "\tbatch: [ 7200], loss: 1024.914917\n",
      "\tbatch: [ 8000], loss: 1852.923706\n",
      "\tbatch: [ 8800], loss: 974.625732\n",
      "\tbatch: [ 9600], loss: 835.758606\n",
      "\tbatch: [10400], loss: 1250.361694\n",
      "\tbatch: [11200], loss: 1493.898193\n",
      "\tbatch: [12000], loss: 1543.904419\n",
      "\tbatch: [12800], loss: 1225.508301\n",
      "\tbatch: [13600], loss: 2008.802002\n",
      "\tbatch: [14400], loss: 1153.525391\n",
      "\tbatch: [15200], loss: 1303.408813\n",
      "\tbatch: [16000], loss: 1132.909668\n",
      "\tbatch: [16800], loss: 1460.375977\n",
      "\tbatch: [17600], loss: 1271.874878\n",
      "\tbatch: [18400], loss: 1314.172852\n",
      "\tbatch: [19200], loss: 1070.991089\n",
      "\tbatch: [20000], loss: 2087.172607\n",
      "\tbatch: [20800], loss: 1702.593140\n",
      "\tbatch: [21600], loss: 1682.266846\n",
      "\tbatch: [22400], loss: 1338.411621\n",
      "Epoch: 2, escaplse     145.82 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "epochs = 3\n",
    "learning_rate = 1e-3\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "model = LSTMRegressionModel(embedding_dim, hidden_dim, vocab_size)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#for batch, (X, y) in enumerate(data_pipe):\n",
    "#    print(batch, X.size(), y.size())\n",
    "#    pred = model(X)\n",
    "#    print(pred.size())\n",
    "#   break\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train(data_pipe, model, loss_fn, optimizer)\n",
    "    print(f\"Epoch: {epoch}, escaplse {time.time() - epoch_start_time:10.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2_dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
