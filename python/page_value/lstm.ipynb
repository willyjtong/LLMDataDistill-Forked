{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 1.5.3\n",
      "numpy 1.24.3\n",
      "torch 2.0.0+cu117\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "for module in [pd, np, torch]:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "ch = spacy.load('zh_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '爱', '北', '京', '天', '安', '门']\n",
      "['我', '爱', '北京', '天安门']\n"
     ]
    }
   ],
   "source": [
    "def charTokenize(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "print(charTokenize('我爱北京天安门'))\n",
    "\n",
    "def cnTokenize(text):\n",
    "    return [tok.text for tok in ch(text)]\n",
    "\n",
    "print(cnTokenize('我爱北京天安门'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built vocab...\n",
      "<class 'torchtext.vocab.vocab.Vocab'> 7929\n",
      "([1, 25, 278, 349, 598, 97, 225, 335, 2], 2.0)\n",
      "Applied transform...\n",
      "Afte batch ...\n",
      "After seperate batch ...\n",
      "tensor([[   1, 2009, 1457,   71,  111,    7, 2009, 1457,   71,  111,    6,   10,\n",
      "           26,  211,  866,  464,    5,    5,  299,   29,    4,  527,   86, 1365,\n",
      "          477,  123,  462,   52,   49,   10, 1794,   21,   50,    7,    6, 1001,\n",
      "          844, 1649,  363,  994,   16,    4,  849,  152,   97,   72,    6,  778,\n",
      "           21,  156,    4, 1547,  393, 1343,    4,  725,   72,   10,  770,   14,\n",
      "         3551,  240, 2728,    6,  523,  152,  136, 3246,  387,  393,  156,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [   1, 1397,  775,    8,  825,  852,   21,   35,   86,  552,  100,  201,\n",
      "           81,   35,  268,    8, 1714,  592,  339,  434,   28,  934,   50,    8,\n",
      "           83,  265,  825,  852,   21,   35,  277,  809,   70,  391,   86,  552,\n",
      "           35,  268,  268,  121,  579,   81,    8,  228,  760,   35,  740,  293,\n",
      "            7, 1397,  775,   86,  527,    6,   17,   12,   12,   75,   29,  113,\n",
      "          163,  825,  852,   21,   35,  277,  809,   70,  391,   86,  552,   35,\n",
      "          268,  265,  173,  108,   17,   12,   12,   75,   29,  843,  265,  173,\n",
      "          339,  647,  104,  265,    8,   17,   12,   12,   54,   29,  843,  265,\n",
      "           86,  552,   35,  268,  268,  121,  579,   81,   10,  567,    8,   83,\n",
      "          211,  143,  228,  760,   35,  259,   50,    5,  173,   35,   27,  339,\n",
      "          434,    7,    6, 1397,  775,   52,   49,    6,  721,  385,  148,   20,\n",
      "         1185,  239,   36,  558,  239,   40,  646,  332,   15,  533,  667,  265,\n",
      "           85,  251,   60,  178,  281,  700,  646,  332,    9,  809,  349,  377,\n",
      "           86,  552,   48,  216,  339,  434,   20,   84,   96,   55,  408,  157,\n",
      "          128,  408,  222,  230,  809,  349,  377, 1374, 2681,   80,   50,    5,\n",
      "           48,  216,   27,  339,  434,    9,  809,  349,  377,  393,  465, 1072,\n",
      "           28,  530,  196,  539,  339,  434,    9, 1438,  538,  353,  663,   46,\n",
      "           66,  112,   70,  831,   96,  229,    5,  339,  434,    8,   18,  797,\n",
      "           84, 1479,  158,   30,   48,  244,   55,  981,  428,  129,  230,  136,\n",
      "          270,   61,  395,   60,  119,  107,  166,   72, 1252,  281,  700,    5,\n",
      "          228,  760,  225,  106,  112,  339,  434,  134,    9,  136, 1258,  967,\n",
      "          798,  411,  119,   50,  478,  936,  262,  167,    5,   86,  552,   35,\n",
      "           70,  831,  174,  491,  675, 1422,  571,  192,  650,  134,   96,  229,\n",
      "            7,    6,  560,  162,  560,  227,  144,  324,  296,  224,  418,  447,\n",
      "          141,  602, 1108,    8,  123,  835,   48,   83,   58,  190,    5,  779,\n",
      "          448,  144,  324,   14,  456,  541,  250,   14,  380,  791,    8,  590,\n",
      "         1093,  255,   40,  120,   16,  779,  448,  602, 1108,  247,  176,  230,\n",
      "          682,  426,  229,  385,  100,  576,  245,    7,  336,  519,  142,  612,\n",
      "            2]])\n",
      "tensor([90.,  0.])\n",
      "After apply padding ...\n"
     ]
    }
   ],
   "source": [
    "train_data_path = './data/train.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([train_data_path])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='r')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)\n",
    "\n",
    "def extract_attributes(row):\n",
    "    return row[1], row[2]\n",
    "data_pipe = data_pipe.map(extract_attributes)\n",
    "\n",
    "def get_tokens(data_iter, tokenizer):\n",
    "    for _, text in data_iter:\n",
    "            yield tokenizer(text)\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        get_tokens(data_iter, tokenizer),\n",
    "        min_freq=2,\n",
    "        specials=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"],\n",
    "        special_first=True\n",
    "    )\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(data_pipe, charTokenize)\n",
    "print('Built vocab...')\n",
    "print(type(vocab), len(vocab))\n",
    "\n",
    "def getTransform(vocab):\n",
    "    text_transform = T.Sequential(\n",
    "        T.VocabTransform(vocab),\n",
    "        T.AddToken(1, begin=True),\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_transform\n",
    "\n",
    "def apply_transform(sample):\n",
    "    text_transformer = getTransform(vocab)\n",
    "    tokenized_text = charTokenize(sample[1])\n",
    "    return text_transformer(tokenized_text), float(sample[0])\n",
    "\n",
    "print(apply_transform(('2',\"我爱北京天安门\")))\n",
    "\n",
    "data_pipe = data_pipe.map(apply_transform)\n",
    "print('Applied transform...')\n",
    "#for sample in data_pipe:\n",
    "#    print(sample)\n",
    "#    break\n",
    "\n",
    "def sortBucket(bucket):\n",
    "    return sorted(bucket, key=lambda x: len(x[0]))\n",
    "\n",
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 2, \n",
    "    batch_num=5,  # batch_num is the number of batches to keep in a bucket\n",
    "    bucket_num=1, # bucket_num is the number of buckets\n",
    "    use_in_batch_shuffle=False, \n",
    "    sort_key=sortBucket\n",
    ")\n",
    "\n",
    "print('Afte batch ...')\n",
    "#for batch in data_pipe:\n",
    "#    print(batch)\n",
    "#    print(len(batch))\n",
    "#    break\n",
    "\n",
    "\n",
    "def separate_batch(batch):\n",
    "    '''\n",
    "    Inputs: [(text1, label1), (text2, label2), ...]\n",
    "    Outputs: ([text1, text2, ...], [label1, label2, ...])\n",
    "    '''\n",
    "    texts, labels = zip(*batch)\n",
    "    return texts, labels\n",
    "\n",
    "data_pipe = data_pipe.map(separate_batch)\n",
    "print('After seperate batch ...')\n",
    "#for texts, labels in data_pipe:\n",
    "#    print(len(texts), texts)\n",
    "#    print(len(labels), labels)\n",
    "#    break\n",
    "\n",
    "\n",
    "def apply_padding(sample):\n",
    "    return (T.ToTensor(0)(list(sample[0])), torch.tensor(list(sample[1])))\n",
    "\n",
    "data_pipe = data_pipe.map(apply_padding)\n",
    "print('After apply padding ...')\n",
    "for texts, labels in data_pipe:\n",
    "    print(len(texts), texts)\n",
    "    print(len(labels), labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressionModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, embedding_pretrained=None) -> None:\n",
    "        super(LSTMRegressionModel, self).__init__()\n",
    "        if embedding_pretrained is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_pretrained, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-2)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [batch_size, seq_len] -> [batch_size, seq_len, embed_dim]\n",
    "        x = self.embedding(inputs)\n",
    "        # [batch_size, seq_len, embed_dim] -> [batch_size, seq_len, hidden_dim]\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        # [batch_size, seq_len, hidden_dim] -> [batch_size, hidden_dim]\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(dataset, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataset, 0):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}]\")\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "model = LSTMRegressionModel(embedding_dim, hidden_dim, vocab_size)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2_dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
